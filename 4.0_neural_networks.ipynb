{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'use strict'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var max = require('lodash/max');\n",
    "var findIndex = require('lodash/findIndex');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks & Deep[ish] Learning\n",
    "\n",
    "There are a few js neural network libraries around.\n",
    "\n",
    " - [synaptic](https://github.com/cazala/synaptic)\n",
    " - [convnetjs](http://cs.stanford.edu/people/karpathy/convnetjs/)\n",
    " - [char-rnn](https://github.com/garywang/char-rnn.js)\n",
    " - [neuro.js](https://github.com/janhuenermann/neurojs)\n",
    " - even a proposal for [tensorflow](https://github.com/node-tensorflow/node-tensorflow) in node!\n",
    " \n",
    " \n",
    "We re going to focus on CONVNET.JS, built y folks at Standford its mature and stable. Although, synaptic looks great too and we will be borroeing from a lot of their excellent introductory docs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Networks 101\n",
    "\n",
    "Let's look at the first part of [synaptics](https://github.com/cazala/synaptic/wiki/Neural-Networks-101) 101 guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "As we saw a single neuron in a neural network encodes a weight vector and reduces applies this to a input vector, reducing it to a single value. This is the projection of the input on the weights within the n-dimensional weight space.\n",
    "\n",
    "This reduced quantity is then put through an activation function on order to produce the neuron's output activation. \n",
    "\n",
    "There are many possible [activation functions](https://en.wikipedia.org/wiki/Activation_function) and any library you use will likely have multiple choices and thre is always scope to code your own.\n",
    "\n",
    "Network convergence and stability can depend significantly on the choice ecause of the following:\n",
    "\n",
    " - blow up\n",
    " - [vanishing gradients](https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Functions\n",
    "\n",
    "Right back atthe start of the day we looked at loss and error functions. ConvNET docs talk about `loss layers` these layers of the network have an in built loss function that is used during training. ConvNET doesn't expose us to that detail however it's there is some other terminology in plat that it's worth mentioning:\n",
    "\n",
    " - softmax\n",
    " - SVM\n",
    " - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convnetjs\n",
    "\n",
    "Convnet introduces for following main classes:\n",
    "\n",
    " - [Vol](https://github.com/karpathy/convnetjs/blob/master/src/convnet_vol.js) - a 3d volume internally organised as a list\n",
    " - [Layer](https://github.com/karpathy/convnetjs/tree/master/src) - a layer in a network. There are different types of layer; input, fc (fully connected), loss (softmax)\n",
    " - [Net](https://github.com/karpathy/convnetjs/blob/master/src/convnet_net.js) - the network itself, consisting of layers. Net is responsible for pushing data `forward` through the layers to produce an output. During training and backpropgation it is responsile for calling the `backward` function to calculate gradients.\n",
    " - [Trainer](https://github.com/karpathy/convnetjs/blob/master/src/convnet_trainers.js) - takes a network, parameters, examples and the associated correct labels and it will train the network\n",
    "\n",
    "\n",
    "The simplest possile Network 2D input, 1 output class and train it with the XOR function\n",
    "\n",
    "\n",
    " A | B | Out\n",
    "---|---|---:\n",
    " 0 | 0 | 0 \n",
    " 1 | 0 | 1 \n",
    " 0 | 1 | 1 \n",
    " 1 | 1 | 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'use strict'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var convnetjs, {Vol, Net, Layer, Trainer} = require('convnetjs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data\n",
    "\n",
    "Lets setup a full training dataset. Which is easy as we only have 4 cases, each input vector needs ot be packed within a `Vol`\n",
    "\n",
    "From the [docs](http://cs.stanford.edu/people/karpathy/convnetjs/docs.html):\n",
    "\n",
    "    // create a Vol of size 32x32x3, and filled with random numbers\n",
    "    var v = new convnetjs.Vol(32, 32, 3);\n",
    "    var v = new convnetjs.Vol(32, 32, 3, 0.0); // same volume but init with zeros\n",
    "    var v = new convnetjs.Vol(1, 1, 3); // a 1x1x3 Vol with random numbers\n",
    "\n",
    "    // you can also initialize with a specific list. E.g. create a 1x1x3 Vol:\n",
    "    var v = new convnetjs.Vol([1.2, 3.5, 3.6]);\n",
    "\n",
    "\n",
    "\n",
    "NOTE: Remember to draw this out on the whiteboard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var X = [\n",
    "    new Vol([0,0]),\n",
    "    new Vol([1,0]),\n",
    "    new Vol([0,1]),\n",
    "    new Vol([1,1])\n",
    "];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is maintained within a property `w` on each vol and the gradients associated with these in `dw`. Check the value of these for our training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ '0', '1' ]\n"
     ]
    }
   ],
   "source": [
    "// console out the innards of a Vol here\n",
    "console.log(Object.keys(X[0].dw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need to show the network correct output values, so we need an output array, which is a plain list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var Y = [0,1,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var layer_defs = [];\n",
    "\n",
    "layer_defs.push({type:'input', out_sx:1, out_sy:1, out_depth:2});\n",
    "\n",
    "layer_defs.push({type:'softmax', num_classes:2});\n",
    "\n",
    "// create a net\n",
    "var net = new Net();\n",
    "net.makeLayers(layer_defs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for class 0 is assigned: 0.6777023643113648\n",
      "score for class 1 is assigned: 0.3222976356886352\n",
      "Predicted Class:  0\n"
     ]
    }
   ],
   "source": [
    "var scores = net.forward(X[1]); // pass forward through network\n",
    "\n",
    "// scores is now a Vol() of output activations\n",
    "console.log('score for class 0 is assigned: ' + scores.w[0]);\n",
    "console.log('score for class 1 is assigned: ' + scores.w[1]);\n",
    "var maxp = max(scores.w);\n",
    "console.log(\"Predicted Class: \", findIndex(scores.w, s => s === maxp));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Training\n",
    "\n",
    "Now we are going to train the network with a single pass through our training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ fwd_time: 0,\n",
       "  bwd_time: 0,\n",
       "  l2_decay_loss: 7.829163779883886e-7,\n",
       "  l1_decay_loss: 0,\n",
       "  cost_loss: 0.6981250709291885,\n",
       "  softmax_loss: 0.6981250709291885,\n",
       "  loss: 0.6981258538455665 }"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var trainer = new Trainer(net, { learning_rate:0.01, l2_decay:0.001 });\n",
    "\n",
    "for (var i = 0; i < 10000; i++) {\n",
    "trainer.train(X[0], 0);\n",
    "trainer.train(X[1], 1);\n",
    "trainer.train(X[2], 1);\n",
    "trainer.train(X[3], 0);\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the layer definitions. In this first try, let's use the simplest possible network; 2 layers. 1x `input` layer and 1x loss layer for which we will use a softmax.\n",
    "\n",
    "Try runing the following cell, \n",
    " - what do you think of the output scores?\n",
    " - try different memers of our training set X[n]\n",
    " - try running the cell multiple times with the input as X[1], what do you notice? do you know why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability that x is class 0: 0.49753693886314754\n",
      "probability that x is class 1: 0.5024630611368525\n"
     ]
    }
   ],
   "source": [
    "var scores2 = net.forward(X[0]);\n",
    "console.log('probability that x is class 0: ' + scores2.w[0]);\n",
    "console.log('probability that x is class 1: ' + scores2.w[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function test(net, X, Y) {\n",
    "    X.map((x, idx) => {\n",
    "        var scores = net.forward(x);\n",
    "        var maxp = max(scores.w);\n",
    "        var c = findIndex(scores.w, s => s === maxp);\n",
    "        var rw = Y[idx] === c ? \"RIGHT\" : \"WRONG\"\n",
    "        console.log(\"Pred:\", c, \"Actual:\", Y[idx], \"Prob:\", maxp, rw)\n",
    "    });\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: 1 Actual: 0 Prob: 0.5024630611368525 WRONG\n",
      "Pred: 1 Actual: 1 Prob: 0.5014116065045738 RIGHT\n",
      "Pred: 1 Actual: 1 Prob: 0.5012767524559826 RIGHT\n",
      "Pred: 1 Actual: 0 Prob: 0.5002252844101499 WRONG\n"
     ]
    }
   ],
   "source": [
    "test(net, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of these results? great? poor?\n",
    "\n",
    "Unless you are very luck these should be awful! cal you think of some reasons why?\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "jp-Babel (Node.js)",
   "language": "babel",
   "name": "babel"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "8.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
